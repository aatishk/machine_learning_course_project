---
title: "Practical Machine Learning MOOC Assignment"
author: "Aatish Kumar"
date: "24 Oct 2014"
output: html_document
keep_md: true
---

### Background 
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. 

In this study, there were six participants and data from accelerometers on the belt, forearm, arm, and dumbell of all the participants were collected. The participants were asked to perform barbell lifts correctly and incorrectly in 5 different ways. The goal of the project is to predict the manner in which they did the exercise. 

### Data sources

The data from this project came from [Human Activity Recognition](http://groupware.les.inf.puc-rio.br/har) project.

The training data is available from: [Training Data](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv)

The testing data is avaiable from: [Testing Data](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv)

### Data cleaning

First, we read the csv files for the training and test data sets with *na.strings* set to appropriate values.

```{r read_data, cache=TRUE}
# read the train and test data
train_data <- read.csv("pml-training.csv", na.strings = c("", "NA", "#DIV/0!"))
test_data <- read.csv("pml-testing.csv", na.strings = c("", "NA", "#DIV/0!"))
```

Next, we do these three steps to clean up the data.

1. Get rid of the first column as it's a row count.
2. Remove the predictors which have very small variance.
3. Remove the predictors which have 75% or more missing values (NA's).

```{r clean_data, cache=TRUE}
# number of columns in raw training data set
ncol(train_data)

# 1. remove first column (it's a row count)
train_data <- train_data[-1]

# require caret package for nearZeroVar function
require(caret)

# 2. identify predictors with near zero variance
near_zero_variance_preds <- nearZeroVar(train_data, saveMetrics = TRUE)

# 2. remove predictors with near zero variance from training dataset
train_data <- train_data[, !near_zero_variance_preds$nzv]

# 3. identify predictors with 75% missing values
missing_value_preds <- sapply(colnames(train_data), 
                              function(x) if(sum(is.na(train_data[, x])) > 0.75*nrow(train_data))
                                            return(TRUE)
                                          else
                                            return(FALSE)
                              )

# 3. remove predictors with 75% missing values from training dataset
train_data <- train_data[, !missing_value_preds]

# number of columns in cleaned-up training data set
ncol(train_data)
```
### Model creation
We used [Random Forest](http://en.wikipedia.org/wiki/Random_forest) model to fit the training data set. This method combines the concepts of "bagging" and "random selection of features" in order to generate a collection of decision trees with controlled variance. To predict the performance of generated model, we use 10-fold cross-validation.
```{r model_creation, cache=TRUE}
# fit random forests model
# 10-fold cross validation to predict classe with all the predictors left after data cleaning

# randomForest package 
require(randomForest)

# set seed for reproducibility
set.seed(123456)

# start the model fitting
model.rf <- train(classe ~ ., method = "rf", data = train_data, importance = TRUE, 
                  trControl = trainControl(method = "cv", number = 10)
                  )

```

### Model performance
We test the model performance by analyzing the contributions of predictors to the model and using the confusion matrix.

```{r model_perf, cache=TRUE, fig.height = 30}
# print details of the fitted model
summary(model.rf)

# plot the different predictors' contributions to fitted model
plot(varImp(model.rf))

# print details of performance of fitted model
model.rf$finalModel
```
As we can note, the OOB estimate of error rate is given by the 10 fold cross-validated model as 0.04%. The *out of sample* error can be calculated by calculating the ratio of number of miss-classifications to total number of classifications. This ratio is given as (1+3+2+1+1)/(5581+3798+3421+3215+3707) = 0.04% which agrees with the OOB estimate of error.

### Model prediction
Finally, we use the model to predict the *classe* variable for the given test cases. We also store the results in separate files in order to submit them in the next part of the assignment. All the 20 cases are predicted correctly.

```{r model_pred, cache=TRUE}
# do the prediction on the test data set
predicted <- predict(model.rf, newdata=test_data)

# print the predicted values
print(predicted)

# store the predicted values in separate files to be uploaded
# function to write output file
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("./prediction/problem_id_", i, ".txt")
    write.table(x[i], file = filename, quote = FALSE, row.names = FALSE, col.names = FALSE)
  }
}

# call function for each of the predicted values
pml_write_files(predicted)
```